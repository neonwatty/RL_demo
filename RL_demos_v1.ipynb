{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-learning process complete\n"
     ]
    }
   ],
   "source": [
    "from my_gridworld import my_gridworld\n",
    "test = my_gridworld()\n",
    "test.qlearn(gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 0, 0);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 255, 0);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.color_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 0, 0);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "starting_location = [0,2]\n",
    "test.animate_movement(starting_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          up      down     left     right    still\n",
      "00    -1.000    -1.000    7.812     1.953    3.906\n",
      "01    -1.000     3.906    3.906     3.906    1.953\n",
      "02    -1.000     1.953   -1.000     7.812    3.906\n",
      "03    -1.000     3.906   15.625    15.625    7.812\n",
      "04    -1.000     7.812   31.250    31.250   15.625\n",
      "05    -1.000    15.625   62.500    -1.000   31.250\n",
      "10     3.906    -1.000   15.625     3.906    7.812\n",
      "11     1.953     7.812   -1.000    -1.000    3.906\n",
      "13     7.812    -1.000   -1.000    31.250   15.625\n",
      "14    15.625    15.625   62.500    62.500   31.250\n",
      "15    31.250    31.250  125.000    -1.000   62.500\n",
      "20     7.812    -1.000   31.250    -1.000   15.625\n",
      "22    -1.000    -1.000  125.000    -1.000   62.500\n",
      "24    31.250    -1.000   -1.000   125.000   62.500\n",
      "25    62.500    62.500  250.000    -1.000  125.000\n",
      "30    15.625    -1.000   62.500    62.500   31.250\n",
      "31    -1.000    31.250  125.000   125.000   62.500\n",
      "32    62.500    62.500  250.000   250.000  125.000\n",
      "33    -1.000   125.000  500.000    -1.000  250.000\n",
      "35   125.000    -1.000  500.000    -1.000  250.000\n",
      "40    31.250    -1.000   31.250   125.000   62.500\n",
      "41    62.500    62.500   62.500   250.000  125.000\n",
      "42   125.000   125.000  125.000   500.000  250.000\n",
      "43   250.000   250.000  250.000  1000.000  500.000\n",
      "44     0.000     0.000    0.000     0.000    0.000\n",
      "45   250.000  1000.000  250.000    -1.000  500.000\n",
      "50    62.500    -1.000   -1.000    62.500   31.250\n",
      "51   125.000    31.250   -1.000   125.000   62.500\n",
      "52   250.000    62.500   -1.000   250.000  125.000\n",
      "53   500.000   125.000   -1.000   500.000  250.000\n",
      "54  1000.000   250.000   -1.000   250.000  500.000\n",
      "55   500.000   500.000   -1.000    -1.000  250.000\n"
     ]
    }
   ],
   "source": [
    "test.show_qmat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking in detail at a class control problem - cart pole - lets look at a simpler problem typically called 'grid world'.  \n",
    "\n",
    "This is a simplified version of the shortest path problem - a problem often solved in video games (this is how enemy AI units find you in the game), mapping services (finding the shortest route from A to B), and robot path planning (e.g., for a cleaning robot) - get a user from its starting location to a desired destination in as few steps as possible.    \n",
    "\n",
    "There are many algorithms specifically designed to solve just this task - the most popular being [Dijkstraâ€™s and A* algorithms](http://www.redblobgames.com/pathfinding/a-star/introduction.html).  However here we will use the flexible RL framework as it too provides great results, and the (relative) simplicity of the problem will allow us to illustrate the more general RL learning process hopefully clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toy problem we have a grid like the one illustrated below.  Each square tile is a location in the world.  Here the black square denotes the user, the green square the desired destination, and the blue squares impenetrable obstacles (the user cannot move to).  \n",
    "\n",
    "The actions available to our user are to move one unit left, right, up, or down, or stay still, and it can move to any free square (here colored magenta) or the goal (colored green).  If the user ever moves to the goal square the game is over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning: components\n",
    "\n",
    "How can we train our agent to move the user square effeciently to the goal, regardless of the user's initial location?  In other words, how can we teach the agent the right action to take at each state (here state = location on the grid)?  Lets go over this at a high level, in 3 steps.\n",
    "\n",
    "Remember the sort of information the agent deals with, and the control (actions) it has over the user.  \n",
    "\n",
    "\n",
    "- The agent is aware of the current **state** of the enviroment - what the agent 'sees', the information it receives, at each step.  In *grid world* this is just the location of the user square.  Generally speaking - we decide the type of information that constitutes the state and in practice this depends on what information we think is reasonably available (to give to our agent).  \n",
    "\n",
    "\n",
    "- The agent can then take an **action** - the set of actions is typically determined by the problem environment.  In *grid world*, for example, the agent can move the user square adjacently **one unit** up/down/left/right or keep it still.  Another example, in an autopilot control problem the range of actions is completely defined by the available range of motions of the machine being controlled.\n",
    "\n",
    "\n",
    "- Once this action is taken the agent receives a **reward** - based on where the action took the user square (the new state of the user).  We decide what the rewards look like (not the agent itself), which is how we communicate our goal to the agent, and we want a reward for a given action to be **larger** for those actions that get us closer to accomplishing our goal (and less for those actions which do not).  In *grid world* we assign a negative value like -1 to all actions (one unit movement) which leads to a non-goal state, and a large positive number like 1000 to actions leading to the goal state itself.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the components fit together\n",
    "\n",
    "Note the sequence of events taken by the agent consists of a sequence of steps -\n",
    "\n",
    "\n",
    "**step 1:** start at state 0 $s_0$ --> take action 1 $a_1$ --> move to state 1 $s_1$ + recieve reward 1 $r_1$\n",
    "\n",
    "**step 2:** start at state 1 $s_1$ --> take action 2 $a_2$--> move to state 2 $s_2$ + recieve reward 2 $r_2$\n",
    "\n",
    "**step 3:** start at state 2 $s_2$ --> take action 3 $a_3$ --> move to state 3 $s_3$ + receive reward 3 $r_3$\n",
    "\n",
    "**step 4:** ...\n",
    "\n",
    "or in short the first three steps look like\n",
    "\n",
    "($s_0$, $a_1$, $r_1$), ($s_1$, $a_2$, $r_2$), ($s_2$, $a_3$, $r_3$), ($s_3$,...\n",
    "\n",
    "and once we tune the agent correctly we want it to choose the best action at each step of this process, to *maximize* its reward.\n",
    "\n",
    "In other words we want to teach our agent the best *function* from states --> actions, whose chosen actions maximize the rewards the agent recieves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent by Q learning\n",
    "\n",
    "How do we do this - train the agent to take the proper actions at each state to **maximize its total reward**?  Calling $r_k = r(s_{k-1},a_k,s_k)$ the reward at the $k^{th}$ step by taking action $a_k$ and moving from state $s_{k-1}$ to $s_k$, mathematically we want to maximize the sequence of rewards\n",
    "\n",
    "$Q(s_0,a_1) = r_1 + r_2 + r_3 + ... $\n",
    "\n",
    "\n",
    "This is a cost function that - in the case of grid world - we would like to be maximal for every possible initial state $s_0$.  Because the number of states and actions is finite $Q$ is a just a matrix of size\n",
    "\n",
    "$Q$ is an (number of states) x (number of actions) dimensional matrix\n",
    "\n",
    "We cannot expect to directly apply the conventional tools of nonlinear optimization (e.g., stochastic gradient descent) to maximize $Q(s_0,a_1)$, as e.g., both inputs are discrete.\n",
    "\n",
    "### Another common Q-learning cost function\n",
    "\n",
    "Another common cost function used to maximize these rewards dampens later rewards using a controller $\\gamma \\in [0,1]$ to lessen the effect of future state rewards. \n",
    "\n",
    "$Q(s_0,a_1) = r_1 + \\gamma^1r_2 + \\gamma^2r_3 + ... $\n",
    "\n",
    "By scaling $\\gamma$ up and down we can lessen the contribution of future rewards.  For example\n",
    "\n",
    "- When we set $\\gamma = 0$ then only the first reward remains $r_1$.   Maximizing $Q$ means we maximize the reward given by the first step of the process, for all states.  Our agent learns to take a 'greedy' approach to accomplishing our goal, at each state taking the next step that maximizes the next step reward only.\n",
    "\n",
    "\n",
    "- When we set $\\gamma = 1$ then we all the $\\gamma$'s disappear and we have our original cost function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate maximization via recursion \n",
    "\n",
    "How can we maximize\n",
    "\n",
    "$Q(s_0,a_1) = r_1 + r_2 + r_3 + ... $\n",
    "\n",
    "in practice?  Call this maximum (that is, the largest sum of rewards after recieving $r_1$ from making decision $a_1$ at $s_0$) $Q^*(s_0,a_1)$.  The trick here is that this definition is *recursive*, that is\n",
    "\n",
    "$Q^*(s_0,a_1) = r_1 + Q^*(s_1,a_2)$\n",
    "\n",
    "This recursive version is sometimes called *Bellman's equation*. \n",
    "\n",
    "While this recursive definition doesn't help us to optimize $Q^*$ *directly*, we can use it to apply a heuristic in order to *approximately* maximize this quantity.  In short: we run through the gamat of possible initial states and update $Q^*$ by trial-and-error interactions with the enviroment.  We run a (large number of) simulations - called *episodes* in RL jargon - which en masse help carve out the right set of actions for our agent to take.  \n",
    "\n",
    "Here's how we do it - we run over a large sequence of episodes where we allow our agent to run the user around towards the goal from various initial states, and update $Q^*$ as we go along.  For each episode we:\n",
    "\n",
    "1.  Initialize $Q^*$ \n",
    "2.  Select a random initial state $s_0$\n",
    "3.  While the goal state has not been reached we \n",
    "    -  select a random action at our current state $s_k$\n",
    "    -  update our approximation \n",
    "    \n",
    "    $Q^*(s_k,a_{k+1}) = r(s_k,a_{k+1}) + Q^*(s_{k+1},a_{k+2})$\n",
    "    \n",
    "    -  update state $s_k$ --> $s_{k+1}$ given action $a_k$\n",
    "    \n",
    "Eventually, as we cycle through the entire set of initial states through episodes this will converge to the true value of $Q^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the agent move intelligently given a learned Q?\n",
    "\n",
    "Given $Q^*$ learned properly, the agent moves intelligently at a given state $s_k$ by taking the action that maximizes $Q^*$ there.  i.e., \n",
    "\n",
    "$a_k = \\underset{a}{\\operatorname{argmax}} Q^*(s_k,a_{}) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = BlockGrid(5, 5, fill=(234, 123, 234))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obstacles = [[1,2],[3,4],[2,3]]  # impenetrable obstacle locations\n",
    "\n",
    "states = []\n",
    "for i in range(grid.height):\n",
    "    for j in range(grid.width):\n",
    "        block = [i,j]\n",
    "        if block not in obstacles:\n",
    "            states.append(str(i) + str(j))\n",
    "        \n",
    "# find state-index\n",
    "state_ind = states.index((str(3) + str(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind = np.argwhere(np.asarray((str(3) + str(2))) in np.asarray(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
